# -*- coding: utf-8 -*-
"""Linear_Regression_Scratch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eiuB25FR8mEzXZnP6vsBZkn2XhEXxe-J

# Linear Regression Model
"""

# Importing required libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

"""### Data Preprocessing"""

# Loading the dataset
data = np.genfromtxt("data2.txt", delimiter =',')
print("n x f matrix of training data :",data.shape)

# Separating features and target
X = data[:,0]
y = data[:,1]
print("Length of X = ",len(X))
print("Length of y = ",len(y))

"""### 1. Plot the data (i.e., x-axis for the 1st column, y-axis for the 2nd column)



"""

# Plotting the data
plt.style.use('seaborn-whitegrid')
plt.plot(X,y,'ro')

"""### 2. Normal Equation

The normal equation for linear regression model using MSE.


![Screenshot 2023-10-04 at 3.48.41â€¯PM.png](data:image/png;base64,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)
"""

# Function for normal equation

def normal_equation(X, y):
  X_transpose = np.transpose(X)
  X_Xtranspose = np.dot(X_transpose, X)
  y_Xtranspose = np.dot(X_transpose, y)

  w = np.linalg.solve(X_Xtranspose, y_Xtranspose)

  return w

# Add a column of ones to X for the intercept term
X_with_intercept = np.c_[np.ones((X.shape[0], 1)), X]
# Calculating coefficients using normal equation
w = normal_equation(X_with_intercept,y)
print("Coefficients", w)

"""### 3. Gradient Descent using batch AND stochastic modes respectively

a. Splitting the dataset into training and testing set. (80% Training and 20% Testing)
"""

# Splitting the dataset into training and testing as required.

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)
print("X_train = ",len(X_train))
print("y_train = ",len(y_train))
print("X_test = ",len(X_test))
print("y_test = ",len(y_test))

"""b. Plot MSE vs. iteration of each mode for both training set and testing set (i.e., batch â€“ training
and testing; stochastic â€“ training and testing).

#### Batch Gradient Descent.
In Gradient descent we calculate the mean squared error over all the data samples all together. Hence we take the average over the total number of elements.
"""

def MSE_gd(y_true , y_pred):
  # Calculating cost
  cost = np.sum((y_true - y_pred)**2) / len(y_true)
  return cost

def gradient_descent(X_train,y_train, X_test, y_test, iterations = 100, learning_rate = 0.0001, stopping_threshold = 1e-3):
  # initializing weight, bias, iterations and learning rate
  current_weight = 0.1
  current_bias = 0.01
  iterations = iterations
  learning_rate = learning_rate
  n = float(len(X_train))

  # Initializing storage for cost
  training_costs = []
  testing_costs = []

  previous_cost = None
  # Creating a list for plotting MSE vs iterations
  iter = []
  # Estimation of parameters
  for i in range(iterations):
    iter.append(i)
    # Predictions
    y_pred_train = (current_weight * X_train) + current_bias
    # Calculating current cost
    current_cost = MSE_gd(y_train, y_pred_train)
    # Stopping gradient descent when the difference between current cost and previous cost is less than threshold
    if previous_cost and abs(previous_cost-current_cost) <= stopping_threshold:
      break
    # Updating the cost and weight
    previous_cost = current_cost
    training_costs.append(current_cost)
    # Calculating the gradient
    delta_weight = (2/n) * np.sum(X_train * (y_train - y_pred_train))
    delta_bias = (2/n) * np.sum(y_train - y_pred_train)
    # Updating the weights
    current_weight = current_weight + (learning_rate * delta_weight)
    current_bias = current_bias + (learning_rate * delta_bias)
    # Predicting on testing data
    y_pred_test = (current_weight * X_test) + current_bias
    # MSE for testing predictions
    current_cost_test = MSE_gd(y_test, y_pred_test)
    # Storing testing cost
    testing_costs.append(current_cost_test)
  #print('Iterations required to converge - %d with learning rate - %f'%(len(iter),learning_rate))
  return current_weight, current_bias, training_costs, testing_costs, iter

# Estimating weight and bias using gradient descent using the training data
estimated_weight, estimated_bias, training_costs, testing_costs, iter = gradient_descent(X_train, y_train, X_test, y_test, iterations=1000)
# Plotting MSE vs Iterations
plt.title('MSE vs Iterations')
plt.plot(np.arange(len(training_costs)), training_costs, label='Training', color = 'red')
plt.plot(np.arange(len(testing_costs)), testing_costs, label='Testing', color = 'blue')
plt.xlabel('Iterations')
plt.ylabel('MSE')
# Making predictions using estimated parameters
y_pred = estimated_weight*X + estimated_bias
# Plotting the regression line
plt.figure(figsize = (8,6))
plt.title('Linear Regression function plot')
plt.scatter(X, y, marker='o', color='red')
plt.plot([min(X), max(X)], [min(y_pred), max(y_pred)], color='blue',markerfacecolor='red',markersize=10,linestyle='dashed')
plt.xlabel("X")
plt.ylabel("Y")

plt.show()

"""After plotting the MSE vs Iteration curve for Batch Gradient descent we can see that the global optimum is converged in 231 interations. Also in batch gradient descent, since it considers all the samples to calculate the MSE and udpate the gradient, the curve is smooth and covereges to a local optima which is the global optima in this case.
Batch gradient descent might converge to a local minimum depending on the data.This issue can be addressed using regularized linear models.

The best learning rate would be the one which converges with least number of iterations. Hence, using 0.01 learning rate for Batch Gradient Descent would.

### Stochastic Gradient Descent
In stochastic gradient descent we consider one random sample at a time, calculate it's MSE and then calculate the gradient.
Since we are using one sample at time, the code ran into as issue with the MSE function used above. A separate MSE function is used for Stochastic Gradient Descent.
"""

def MSE_sgd(y_true , y_pred):
  # Calculating cost
  cost = (y_true - y_pred)**2
  return cost

def stochastic_gradient_descent(X_train, y_train, X_test, y_test, iterations = 100, learning_rate = 0.0001, stopping_threshold = 1e-3):
  # initializing weight, bias, iterations and learning rate
  current_weight = 0.1
  current_bias = 0.1
  iterations = iterations
  learning_rate = learning_rate
  n=1
  # Creating a list for plotting MSE vs iterations
  iter_1 = []

  # Initializing storage for cost
  training_costs = []
  testing_costs = []
  previous_cost = None

  for iter in range(iterations):
    iter_1.append(iter)
    # Shuffle the data
    indices = np.random.permutation(len(X_train))
    for i in indices:
      # Predictions
      y_pred_train = (current_weight * X_train[i]) + current_bias
      # Calculating training cost
      current_cost = MSE_sgd(y_train[i], y_pred_train)

      # Calculating the gradient
      delta_weight = (2/n) * (X_train[i] * (y_train[i]-y_pred_train))
      delta_bias = (2/n) * (y_train[i]-y_pred_train)
      # Updating the weights
      current_weight = current_weight + (learning_rate * delta_weight)
      current_bias = current_bias + (learning_rate * delta_bias)
    # Stopping gradient descent when the difference between current cost and previous cost is less than threshold
    if previous_cost and abs(previous_cost-current_cost) <= stopping_threshold:
      break
    # Updating the cost and weight
    previous_cost = current_cost

    # Predicting on testing data
    y_pred_test = (current_weight * X_test) + current_bias
    # MSE for testing predictions
    current_cost_test = MSE_gd(y_test, y_pred_test)
    # Storing testing cost
    testing_costs.append(current_cost_test)
    # Storing training cost
    training_costs.append(current_cost)
  #print('Iterations required to converge',len(iter_1))
  return current_weight, current_bias, training_costs, testing_costs

# Estimating weight and bias using gradient descent using the training data
estimated_weight, estimated_bias, training_costs, testing_costs = stochastic_gradient_descent(X_train, y_train, X_test, y_test, learning_rate = 0.00001, iterations=1000, stopping_threshold = 1e-4)

# Plotting the graphs
plt.title('MSE vs Iterations')
plt.plot(np.arange(len(training_costs)), training_costs, label='Training')
plt.plot(np.arange(len(testing_costs)), testing_costs, label='Testing')
#plt.legend(loc='best')
plt.xlabel('Iterations')
plt.ylabel('MSE')

# Making predictions using estimated parameters
y_pred = estimated_weight*X_train + estimated_bias

# Plotting the regression line
plt.figure(figsize = (8,6))
plt.title('Linear Regression function plot')
plt.scatter(X, y, marker='o', color='red')
plt.plot([min(X), max(X)], [min(y_pred), max(y_pred)], color='blue',markerfacecolor='red',markersize=10,linestyle='dashed')
plt.xlabel("X")
plt.ylabel("Y")

"""After plotting MSE vs iterations, we can see that the gradient is calculated at random samples. The gradient is calculate for every samples using the MSE.
Since random samples are picked at time the plot seems very irratic and takes more iterations to converge as compared to Batch Gradient Descent. Also Stochastic Gradient Descent is very sensitive to learning rate and the plot changes drastically with small variations in the learning rate.

This issue can be solved using variable learning rate. By using momentum we can calculate an optimal learning rate for every randomly picked sample.

##### Plotting the graph for different learning rates.

As we can see stochastic gradient descent is sensitive to learning rate and very difficult to optimize. The gradient mostly shoots past the global minimum after calculating. The best learning rate from the main plot is 0.00001.
"""

learning = [0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.01]
testing_MSE_BGD = []
for i in learning:
  # Estimating weight and bias using gradient descent using the training data
  estimated_weight, estimated_bias, training_costs, testing_costs, iter = gradient_descent(X_train, y_train, X_test, y_test,learning_rate = i, iterations=1000)
  testing_MSE_BGD.append(testing_costs[-1])

learning = [0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.01]
testing_MSE_SGD = []
for i in learning:
  # Estimating weight and bias using gradient descent using the training data
  estimated_weight, estimated_bias, training_costs, testing_costs = stochastic_gradient_descent(X_train, y_train, X_test, y_test,learning_rate = i, iterations=1000)
  testing_MSE_SGD.append(testing_costs[-1])

# Plotting the results
plt.figure(figsize=(8, 6))
plt.plot(learning, testing_MSE_BGD, marker='o', linestyle='-', label='Batch GD')
plt.plot(learning, testing_MSE_SGD, marker='.', linestyle='--', label='Stochastic GD')
plt.xlabel('Learning Rates')
plt.ylabel('MSE of Testing Set')
plt.title('MSE of Testing Set vs Learning Rates')
plt.legend()
plt.show()

# Find the best learning rate
best_lr_bgd = learning[np.argmin(testing_MSE_BGD)]
best_mse_bgd = min(testing_MSE_BGD)

best_lr_sgd = learning[np.argmin(testing_MSE_SGD)]
best_mse_sgd = min(testing_MSE_SGD)

print('Batch Gradient Descent: Best Learning Rate = {}, Best MSE = {}'.format(best_lr_bgd, best_mse_bgd))
print('Stochastic Gradient Descent: Best Learning Rate = {}, Best MSE = {}'.format(best_lr_sgd, best_mse_sgd))

"""### Mini Batch Gradient Descent
To verify the plots for Gradient Descent and Stochastic Gradient Descent, I also tried using Mini Batch Gradient descent where I can specify the size of the batches and see how it affects the MSE vs iterations plot.
"""

def mini_batch_gradient_descent(X_train, y_train, X_test, y_test, iterations = 100,learning_rate = 0.0002, stopping_threshold = 1e-3, batch_size = 50):
  # initializing weight, bias, iterations and learning rate
  current_weight = 0.1
  current_bias = 0.1
  iterations = iterations
  learning_rate = learning_rate
  batch_size = batch_size

  # Storing list
  training_costs = []
  testing_costs = []
  for iter in range(iterations):
    indices = np.random.permutation(len(X_train))
    X_train = X_train[indices]
    y_train = y_train[indices]

    for i in range(0, len(X_train), batch_size):
      X_batch = X_train[i:i+batch_size]
      y_batch = y_train[i:i+batch_size]
      n = batch_size
      # Predictions
      y_pred = (current_weight * X_batch) + current_bias
      # Calculating current cost
      current_cost = MSE_gd(y_batch, y_pred)
      # Calculating the gradient
      delta_weight = (2/n) * np.sum(X_batch * (y_batch - y_pred))
      delta_bias = (2/n) * np.sum(y_batch - y_pred)
      # Updating the weights
      current_weight = current_weight + (learning_rate * delta_weight)
      current_bias = current_bias + (learning_rate * delta_bias)
    # Updating training cost
    training_costs.append(current_cost)

    # Predicting on testing data
    y_pred_test = (current_weight * X_test) + current_bias
    # MSE for testing predictions
    current_cost_test = MSE_gd(y_test, y_pred_test)
    # Storing testing cost
    testing_costs.append(current_cost_test)
  return current_weight, current_bias, training_costs, testing_costs

# Estimating weight and bias using gradient descent using the training data
estimated_weight, estimated_bias, training_costs, testing_costs = mini_batch_gradient_descent(X_train, y_train, X_test, y_test, iterations=1000, learning_rate = 0.001, batch_size = 40)
# Plotting MSE vs Iterations
plt.title('MSE vs Iterations')
plt.plot(np.arange(len(training_costs)), training_costs, label='Training', color = 'red')
plt.plot(np.arange(len(testing_costs)), testing_costs, label='Testing', color = 'blue')
plt.xlabel('Iterations')
plt.ylabel('MSE')

# Making predictions using estimated parameters
y_pred = estimated_weight*X_train + estimated_bias

# Plotting the regression line
plt.figure(figsize = (8,6))
plt.title('Linear Regression function plot')
plt.scatter(X, y, marker='o', color='red')
plt.plot([min(X), max(X)], [min(y_pred), max(y_pred)], color='blue',markerfacecolor='red',markersize=10,linestyle='dashed')
plt.xlabel("X")
plt.ylabel("Y")

"""After plotting the MSE vs iterations curve we can see that the smoothness of the plot changes depending on the size of the batch. Bigger the batch size, smoother is the curve and less sensitive is the model to learning rate.

The optimal batch size I felt was 40 since it calculate the gradient for each batch separately and re iterates making it better than Gradient Descent in terms of accuracy but not better than Stochastic Gradient Descent
"""

