# -*- coding: utf-8 -*-
"""PCA_Scratch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xsWkFM0bAasfg9l3AyjQYnwFmvAe6bN7

# Logistic Regression, Stochastic Gradient, Mini Batch Gradient Descent

### Importing Libraries
"""



# preprocessing libraries
import pandas as pd
import numpy as np
# algorithm libraries
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold

"""### Data Preprocessing"""

data = 'wdbc.data.csv'
names = ['ID', 'diagnosis']
for i in range(1, 31):
    names.append(str(i))
df = pd.read_csv(data, names = names)

df.head()

target = df['diagnosis']
df = df.drop(columns = ['ID', 'diagnosis'])

target.replace('B', -1, inplace = True)
target.replace('M', 1, inplace = True)

target.shape

X_train, X_test, y_train, y_test = train_test_split(df, target, test_size = 0.2)
X_train = X_train.to_numpy()
X_test = X_test.to_numpy()
y_train = y_train.to_numpy()
y_test = y_test.to_numpy()

"""### Logistic Regression"""

def sigmoid(w, x):
    numerator = 1
    denominator = 1 + np.exp(-np.dot(w.T, x))

    return numerator / denominator

def gradient(w, x, y):
    n = x.shape[0]
    summ = 0

    for i in range(n):
        summ += (sigmoid(w, x) - y) * x

    return -summ

"""### Stochastic Gradient Descent"""

def stochastic_gradient(w, x, y):
    n = x.shape[0]
    summ = 0

    for i in range(n):
        summ += (sigmoid(w, x) - y) * x

    return -summ

def sgd(x, y, learning_rate, w, max_epoch=100):
    while max_epoch > 0:
        for i in range(x.shape[0]):
            grad = stochastic_gradient(w, x[i], y[i])
        w = w - grad * learning_rate

        max_epoch = max_epoch - 1

    return w

w = np.zeros((X_train.shape[1]))
w_sgd = sgd(X_train, y_train, 0.01, w)

"""### Mini-Batch Gradient Descent"""

def minibatch_gradient(w, x, y):
    summ = 0

    for i in range(len(x)):
        summ += np.dot((sigmoid(w, x[i]) - y[i]),x[i])

    return summ

def create_mini_batches(x, y, batch_size = 10):
    indices = [i for i in range(x.shape[0])]
    np.random.shuffle(indices)
    mini_batch = []
    i = 0
    while i < x.shape[0]:
        mini_batch.append(indices [i : i + batch_size -1])
        i = i + batch_size

    return mini_batch

def mbgd(x, y, learning_rate, w, max_epoch=100):
    i = 0
    while max_epoch > 0:
        mini = create_mini_batches(x, y, 10)
        for batches in mini:
            x_mini = [x[i] for i in batches]
            y_mini = [y[i] for i in batches]

            mbgrad = minibatch_gradient(w, x_mini, y_mini)
            w = w - learning_rate * mbgrad

        max_epoch = max_epoch - 1
    return w

w = np.zeros((X_train.shape[1]))
w_mbgd = mbgd(X_train, y_train, 0.01, w)

"""### Cross Validation"""

def evaluate(x_train, y_train, x_test, y_test, gradient_descent = 'mbgd'):
    #list for storing folds
    train_fold = list()
    test_fold = list()

    # kfold
    kf = KFold(n_splits = 5)

    # getting indices of data in each fold
    for i, (train_index, test_index) in enumerate(kf.split(x_train)):
        train_fold.append(list(train_index))
        test_fold.append(list(test_index))

    # Calculating weights
    for i in range(len(train_fold)):
        train_data = [x_train[j] for j in train_fold[i]]
        train_labels = [y_train[j] for j in train_fold[i]]

        test_data = [x_train[j] for j in test_fold[i]]
        test_labels = [y_train[j] for j in test_fold[i]]

        weights = np.zeros((x_train.shape[1]))
        wsgd = sgd(np.asarray(train_data), np.asarray(train_labels), 0.01, weights)
        wmbgd = mbgd(np.asarray(train_data), np.asarray(train_labels), 0.01, weights)

        # predicting target and evaluating accuracy
        y_predicted_sgd = []
        y_predicted_mbgd = []
        for x in test_data:
            y_pred_sgd = np.dot(wsgd.T, x)
            y_pred_mbgd = np.dot(wmbgd.T, x)

            if y_pred_sgd > 1:
                y_predicted_sgd.append(1)
            else:
                y_predicted_sgd.append(-1)
            if y_pred_mbgd > 1:
                y_predicted_mbgd.append(1)
            else:
                y_predicted_mbgd.append(-1)
        #Metrics for SGD
        t_pos_sgd = 1
        t_neg_sgd = 1
        f_pos_sgd = 1
        f_neg_sgd = 1

        for i in range(len(y_predicted_sgd)):
            if y_predicted_sgd[i] > 0 and test_labels[i] > 0:
                t_pos_sgd += 1
            if y_predicted_sgd[i] > 0 and test_labels [i] < 0:
                f_pos_sgd += 1
            if y_predicted_sgd[i] < 0 and test_labels[i] > 0:
                f_neg_sgd += 1
            if y_predicted_sgd[i] < 0 and test_labels[i] < 0:
                t_neg_sgd += 1

        # Calculated precision score from confusion matrix
        sgd_prec = []
        sgd_rec = []
        sgd_acc = []

        sgd_rec.append(t_pos_sgd / (t_pos_sgd + f_neg_sgd))
        sgd_prec.append(t_pos_sgd / (t_pos_sgd + f_pos_sgd))
        sgd_acc.append(t_pos_sgd + t_neg_sgd / t_pos_sgd + t_neg_sgd + f_pos_sgd + f_neg_sgd)


        # Metrics for BSGD
        t_pos_mbgd = 1
        t_neg_mbgd = 1
        f_pos_mbgd = 1
        f_neg_mbgd = 1

        for i in range(len(y_predicted_mbgd)):
            if y_predicted_mbgd[i] > 0 and test_labels[i] > 0:
                t_pos_mbgd += 1
            if y_predicted_mbgd[i] > 0 and test_labels [i] < 0:
                f_pos_mbgd += 1
            if y_predicted_mbgd[i] < 0 and test_labels[i] > 0:
                f_neg_mbgd += 1
            if y_predicted_mbgd[i] < 0 and test_labels[i] < 0:
                t_neg_mbgd += 1

        # lists for storing data
        mbgd_prec = []
        mbgd_rec = []
        mbgd_acc = []

        mbgd_rec.append(t_pos_mbgd / (t_pos_mbgd + f_neg_mbgd))
        mbgd_prec.append(t_pos_mbgd / (t_pos_mbgd + f_pos_mbgd))
        mbgd_acc.append(t_pos_mbgd + t_neg_mbgd / t_pos_mbgd + t_neg_mbgd + f_pos_mbgd + f_neg_mbgd)


    if gradient_descent == 'mbgd':
        return mbgd_rec, mbgd_prec, mbgd_acc
    else:
        return sgd_rec, sgd_prec, sgd_acc

#evaluate SGD and MBGD
rec, prec, acc = evaluate(X_train, y_train, X_test, y_test, "sgd")
print("Recall: ", rec)
print("Precision: ", prec)
print("Accuracy: ", acc)
rec_mbgd, prec_mbgd, acc_mbgd = evaluate(X_train, y_train, X_test, y_test, "mbgd")
print("Recall: ", rec_mbgd)
print("Precision: ", prec_mbgd)
print("Accuracy: ", acc_mbgd)

