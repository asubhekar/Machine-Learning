# -*- coding: utf-8 -*-
"""Logistic_Regression_SGD_scratch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LpHLhhPDZnFue72txolz6_rggltQyq4_

# Logistic Regression.

#### For this assignment, you will build 6 models. You need to train Logistic Regression/Regularized Logistic Regression each with Batch Gradient Descent, Stochastic Gradient Descent and Mini Batch Gradient Descent. Also, you should plot their objective values versus epochs and compare their training and testing accuracy. You will need to tune the parameters a little bit to obtain reasonable results.

#### You do not have to follow the following procedure. You may implement your own functions and methods, but you need to show your results and plots.
"""

# Commented out IPython magic to ensure Python compatibility.
# Load Packages
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
# %matplotlib inline
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score

"""# 1. Data processing

- Download the Breast Cancer dataset from canvas or from https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)
- Load the data.
- Preprocess the data.

## 1.1. Load the data
"""

df = pd.read_csv("data-1.csv")

df.head()

# Target labels (ground truth values). After transforming from 'M' and 'B' to -1 and 1.
target = df.iloc[:,1]
target

# Data summary post cleaning.
df.head()

"""## 1.2 Examine and clean data"""

# Some columns may not be useful for the model (For example, the first column contains ID number which may be irrelavant).
# You need to get rid of the ID number feature.
# Also you should transform target labels in the second column from 'B' and 'M' to 1 and -1.

df = df.drop(columns = ['id','Unnamed: 32','diagnosis'])

target.replace('B', 1, inplace = True)
target.replace('M', -1, inplace = True)
target

"""## 1.3. Partition to training and testing sets"""

# You can partition using 80% training data and 20% testing data. It is a commonly used ratio in machine learning.
x_train, x_test, y_train, y_test = train_test_split(df, target, test_size = 0.2)

x_train

"""## 1.4. Feature scaling

Use the standardization to transform both training and test features
"""

# Standardization
import numpy

# calculate mu and sig using the training set
d = x_train.shape[1]
mu = numpy.mean(x_train, axis=0)
sig = numpy.std(x_train, axis=0)

# transform the training features
x_train = (x_train - mu) / (sig + 1E-6)
x_train = x_train.to_numpy()
y_train = y_train.to_numpy()
# transform the test features
x_test = (x_test - mu) / (sig + 1E-6)
x_test = x_test.to_numpy()
y_test = y_test.to_numpy()
print('test mean = ')
print(numpy.mean(x_test, axis=0))

print('\ntest std = ')
print(numpy.std(x_test, axis=0))

"""# 2.  Logistic Regression Model

The objective function is $Q (w; X, y) = \frac{1}{n} \sum_{i=1}^n \log \Big( 1 + \exp \big( - y_i x_i^T w \big) \Big) + \frac{\lambda}{2} \| w \|_2^2 $.

When $\lambda = 0$, the model is a regular logistic regression and when $\lambda > 0$, it essentially becomes a regularized logistic regression.
"""

# Calculate the objective function value, or loss
# Inputs:
#     w: weight: d-by-1 matrix
#     x: data: n-by-d matrix
#     y: label: n-by-1 matrix
#     lam: regularization parameter: scalar
# Return:
#     objective function value, or loss (scalar)
def objective(w, x, y, lam):
    n = x.shape[0]
    sum_1 = 0
    regularize = (lam/2) * ((np.linalg.norm(w))**2)
    for i in range(n):
        prod = np.dot(y[i]*x[i].T,w)
        sum_1 = sum_1 + np.log(1 + np.exp(-prod))
    sum_2 = sum_1 / n
    obj = sum_2 + regularize

    return obj

"""# 3. Numerical optimization

## 3.1. Gradient descent

The gradient at $w$ for regularized logistic regression is  $g = - \frac{1}{n} \sum_{i=1}^n \frac{y_i x_i }{1 + \exp ( y_i x_i^T w)} + \lambda w$
"""

# Calculate the gradient
# Inputs:
#     w: weight: d-by-1 matrix
#     x: data: n-by-d matrix
#     y: label: n-by-1 matrix
#     lam: regularization parameter: scalar
# Return:
#     g: gradient: d-by-1 matrix

def gradient(w, x, y, lam):
    n = x.shape[0]
    sum_1 = 0
    regularize = lam * w
    for i in range(n):
        prod = np.dot(y[i]*x[i].T, w)
        sum_1 += (y[i]*x[i]) / (1 + np.exp(prod))
    sum_2 = -sum_1 / n
    grad = sum_2 + regularize

    return grad

# Gradient descent for solving logistic regression
# You will need to do iterative processes (loops) to obtain optimal weights in this function

# Inputs:
#     x: data: n-by-d matrix
#     y: label: n-by-1 matrix
#     lam: scalar, the regularization parameter
#     learning_rate: scalar
#     w: weights: d-by-1 matrix, initialization of w
#     max_epoch: integer, the maximal epochs
# Return:
#     w: weights: d-by-1 matrix, the solution
#     objvals: a record of each epoch's objective value

def gradient_descent(x, y, lam, learning_rate, w, max_epoch=100):
    obj_val = []
    while max_epoch > 0:
        w = w - (learning_rate * gradient (w, x, y, lam))
        obj_val.append(objective(w, x, y, lam))
        max_epoch = max_epoch - 1
    return w, obj_val

"""Use gradient_descent function to obtain your optimal weights and a list of objective values over each epoch."""

# Train logistic regression
# You should get the optimal weights and a list of objective values by using gradient_descent function.
w1 = np.zeros((x_train.shape[1]))
w1 , obj_val1 = gradient_descent(x_train, y_train, 0, 0.1, w1)
print("Weights = ", w1)
print("Objective Values = ", obj_val1)

# Train regularized logistic regression
# You should get the optimal weights and a list of objective values by using gradient_descent function.
w1_reg = np.zeros((x_train.shape[1]))
w1_reg , obj_val1_reg = gradient_descent(x_train, y_train, 0.5, 0.1, w1_reg)
print("Weights = ", w1_reg)
print("Objective Values = ", obj_val1_reg)

"""## 3.2. Stochastic gradient descent (SGD)

Define new objective function $Q_i (w) = \log \Big( 1 + \exp \big( - y_i x_i^T w \big) \Big) + \frac{\lambda}{2} \| w \|_2^2 $.

The stochastic gradient at $w$ is $g_i = \frac{\partial Q_i }{ \partial w} = -\frac{y_i x_i }{1 + \exp ( y_i x_i^T w)} + \lambda w$.

You may need to implement a new function to calculate the new objective function and gradients.
"""

# Calculate the objective Q_i and the gradient of Q_i
# Inputs:
#     w: weights: d-by-1 matrix
#     xi: data: 1-by-d matrix
#     yi: label: scalar
#     lam: scalar, the regularization parameter
# Return:
#     obj: scalar, the objective Q_i
#     g: d-by-1 matrix, gradient of Q_i

def stochastic_objective_gradient(w, xi, yi, lam):
    # Objective Values
    Q = np.log(1 + np.exp(np.dot(-yi * xi.T, w)))
    regularizer_obj = (lam / 2) * (w**2)
    Q = Q + regularizer_obj

    # gradient
    g = -(yi * xi) / (1 + np.dot(yi * xi.T, w))
    regularizer_grad = lam * w
    g = g + regularizer_grad
    return g, Q

"""Hints:
1. In every epoch, randomly permute the $n$ samples.
2. Each epoch has $n$ iterations. In every iteration, use 1 sample, and compute the gradient and objective using the ``stochastic_objective_gradient`` function. In the next iteration, use the next sample, and so on.
"""

# SGD for solving logistic regression
# You will need to do iterative process (loops) to obtain optimal weights in this function

# Inputs:
#     x: data: n-by-d matrix
#     y: label: n-by-1 matrix
#     lam: scalar, the regularization parameter
#     learning_rate: scalar
#     w: weights: d-by-1 matrix, initialization of w
#     max_epoch: integer, the maximal epochs
# Return:
#
#     w: weights: d-by-1 matrix, the solution
#     objvals: a record of each epoch's objective value
#     Record one objective value per epoch (not per iteration)

def sgd(x, y, lam, learning_rate, w, max_epoch=100):
    obj_val = []
    while max_epoch > 0:
        for i in range(x.shape[0]):
            grad, obj = stochastic_objective_gradient(w, x[i], y[i], lam)
        w = w - grad * learning_rate
        obj_val.append(np.mean(obj))

        max_epoch = max_epoch - 1

    return w, obj_val

"""Use sgd function to obtain your optimal weights and a list of objective values over each epoch."""

# Train logistic regression
# You should get the optimal weights and a list of objective values by using gradient_descent function.
w2 = np.zeros((x_train.shape[1]))
w2, obj_val2 = sgd(x_train, y_train, 0, 0.1, w2)
print("Weights = ", w2)
print("Objective Values = ", obj_val2)
print(len(obj_val2))

# Train regularized logistic regression
# You should get the optimal weights and a list of objective values by using gradient_descent function.
w2_reg = np.zeros((x_train.shape[1]))
w2_reg, obj_val2_reg = sgd(x_train, y_train, 0.5, 0.1, w2_reg)
print("Weights = ", w2_reg)
print("Objective Values = ", obj_val2_reg)

"""## 3.3 Mini-Batch Gradient Descent (MBGD)

Define $Q_I (w) = \frac{1}{b} \sum_{i \in I} \log \Big( 1 + \exp \big( - y_i x_i^T w \big) \Big) + \frac{\lambda}{2} \| w \|_2^2 $, where $I$ is a set containing $b$ indices randomly drawn from $\{ 1, \cdots , n \}$ without replacement.

The stochastic gradient at $w$ is $g_I = \frac{\partial Q_I }{ \partial w} = \frac{1}{b} \sum_{i \in I} \frac{- y_i x_i }{1 + \exp ( y_i x_i^T w)} + \lambda w$.

You may need to implement a new function to calculate the new objective function and gradients.
"""

# Calculate the objective Q_I and the gradient of Q_I
# Inputs:
#     w: weights: d-by-b matrix
#     xi: data: b-by-d matrix
#     yi: label: scalar
#     lam: scalar, the regularization parameter
# Return:
#     obj: scalar, the objective Q_i
#     g: d-by-1 matrix, gradient of Q_i

def mb_objective_gradient(w, xi, yi, lam):
    sum_1 = 0
    sum_2 = 0

    for i in range(len(xi)):
        prod = np.dot(yi[i]*xi[i].T,w)
        sum_1 += np.log(1 + np.exp(-prod))
        sum_2 += (-yi[i]*xi[i]) / (1 + np.exp(prod))

    sum_1 = sum_1 / len(xi)
    regularize_1 = (lam/2) * ((np.linalg.norm(w))**2)
    mbobj = sum_1 + regularize_1

    sum_2 = sum_2 / len(xi)
    regularize_2 = lam * w
    mbgrad = sum_2 + regularize_2

    return mbobj, mbgrad

"""Hints:
1. In every epoch, randomly permute the $n$ samples (just like SGD).
2. Each epoch has $\frac{n}{b}$ iterations. In every iteration, use $b$ samples, and compute the gradient and objective using the ``mb_objective_gradient`` function. In the next iteration, use the next $b$ samples, and so on.
"""

# Create Mini Batches
def create_mini_batches(x, y, batch_size = 10):
    indices = [i for i in range(x.shape[0])]
    np.random.shuffle(indices)
    mini_batch = []
    i = 0
    while i < x.shape[0]:
        mini_batch.append(indices [i : i + batch_size -1])
        i = i + batch_size

    return mini_batch

# MBGD for solving logistic regression
# You will need to do iterative process (loops) to obtain optimal weights in this function

# Inputs:
#     x: data: n-by-d matrix
#     y: label: n-by-1 matrix
#     lam: scalar, the regularization parameter
#     learning_rate: scalar
#     w: weights: d-by-1 matrix, initialization of w
#     max_epoch: integer, the maximal epochs
# Return:
#     w: weights: d-by-1 matrix, the solution
#     objvals: a record of each epoch's objective value
#     Record one objective value per epoch (not per iteration)

def mbgd(x, y, lam, learning_rate, w, max_epoch=100):
    obj_val3 = []
    i = 0
    while max_epoch > 0:
        mini = create_mini_batches(x, y, 320)
        for batches in mini:
            x_mini = [x[i] for i in batches]
            y_mini = [y[i] for i in batches]
            obj_batch = []
            mbobj, mbgrad = mb_objective_gradient(w, x_mini, y_mini, lam)
            w = w - learning_rate * mbgrad
            obj_batch.append(mbobj)
        obj_val3.append(min(obj_batch))
        max_epoch = max_epoch - 1
    return w, obj_val3

"""Use mbgd function to obtain your optimal weights and a list of objective values over each epoch."""

# Train logistic regression
# You should get the optimal weights and a list of objective values by using gradient_descent function.
w3 = np.zeros((x_train.shape[1]))
w3, obj_val3 = mbgd(x_train, y_train, 0, 0.0001, w3)
print("Weights = ", w3)
print("Objective Values = ", obj_val3)
print(len(obj_val3))

# Train regularized logistic regression
# You should get the optimal weights and a list of objective values by using gradient_descent function.
w3_reg = np.zeros((x_train.shape[1]))
w3_reg, obj_val3_reg = mbgd(x_train, y_train, 1, 0.0001, w3_reg)
print("Weights = ", w3_reg)
print("Objective Values = ", obj_val3_reg)
print(len(obj_val3))

"""# 4. Compare GD, SGD, MBGD

### Plot objective function values against epochs.
"""

epochs = [i for i in range(100)]
plt.figure();
plt.plot(epochs, obj_val1_reg, color='darkorange', lw=2, label = 'regularized');
plt.plot(epochs, obj_val1, color = 'green', lw=2, label = 'unregularized')

plt.ylabel('Objective Values');
plt.xlabel('Epochs');
plt.title('Gradient Descent Objective function vs Epochs');
plt.legend();
plt.show();

epochs = [i for i in range(100)]
plt.figure();
plt.plot(epochs, obj_val2_reg, color='darkorange', lw=2, label = 'regularized');
plt.plot(epochs, obj_val2, color = 'green', lw=2, label = 'unregularized')

plt.ylabel('Objective Values');
plt.xlabel('Epochs');
plt.title('Stochastic Gradient Descent Objective function vs Epochs');
plt.legend();
plt.show();

epochs = [i for i in range(100)]
plt.figure();
plt.plot(epochs, obj_val3_reg, color='darkorange', lw=2, label = 'regularized');
plt.plot(epochs, obj_val3, color = 'green', lw=2, label = 'unregularized')

plt.ylabel('Objective Values');
plt.xlabel('Epochs');
plt.title('Mini Batch Gradient Descent Objective function vs Epochs');
plt.legend();
plt.show();

"""# 5. Prediction
### Compare the training and testing accuracy for logistic regression and regularized logistic regression.
"""

# Predict class label
# Inputs:
#     w: weights: d-by-1 matrix
#     X: data: m-by-d matrix
# Return:
#     f: m-by-1 matrix, the predictions
def predict(w, X):
    y_pred = np.zeros(X.shape[0])

    for i in range(X.shape[0]):
        prod = np.dot(w.T,X[i])
        if prod < 0:
            y_pred[i] = -1
        else:
            y_pred[i] = 1
    return y_pred

# evaluate training error of logistic regression and regularized version
y_pred_train= predict(w3, x_train)
y_pred_train_reg = predict(w3_reg, x_train)
error = mean_squared_error(y_train, y_pred_train)
error_reg = mean_squared_error(y_train, y_pred_train_reg)
print("Training error with logistic regression",error)
print("Training error with logistic regression regularized",error_reg)
accuracy = accuracy_score(y_train, y_pred_train)
accuracy_reg = accuracy_score(y_train, y_pred_train_reg)
print("Training accuracy with logistic regression ",accuracy)
print("Training accuracy with logistic regression regularized",accuracy_reg)

# evaluate testing error of logistic regression and regularized version
y_pred_test= predict(w3, x_test)
error= mean_squared_error(y_test, y_pred_test)
y_pred_test_reg = predict(w3_reg, x_test)
error_reg = mean_squared_error(y_test, y_pred_test)
print("Testing error with logistic regression", error)
print("Testing error with logistic regression regularized", error_reg)
accuracy = accuracy_score(y_test, y_pred_test)
accuracy_reg = accuracy_score(y_test, y_pred_test_reg)
print("Testing accuracy with logistic regression", accuracy)
print("Testing accuracy with logistic regression regularized", accuracy_reg)

"""# 6. Parameters tuning

### In this section, you may try different combinations of parameters (regularization value, learning rate, etc) to see their effects on the model. (Open ended question)
"""

lmda = [0, 0.1, 1, 10, 100]
learning = [0.0001, 0.001, 0.01, 0.1]
for i in lmda:
    for j in learning:
        print("Lambda = ", i)
        print("Learning Rate = ", j)
        w3_reg = np.zeros((x_train.shape[1]))
        w3_reg, obj_val3_reg = mbgd(x_train, y_train, i, j, w3_reg)
        y_pred_test= predict(w3, x_test)
        error= mean_squared_error(y_test, y_pred_test)
        y_pred_test_reg = predict(w3_reg, x_test)
        error_reg = mean_squared_error(y_test, y_pred_test)
        print("Testing error with logistic regression", error)
        print("Testing error with logistic regression regularized", error_reg)
        accuracy = accuracy_score(y_test, y_pred_test)
        accuracy_reg = accuracy_score(y_test, y_pred_test_reg)
        print("Testing accuracy with logistic regression", accuracy)
        print("Testing accuracy with logistic regression regularized", accuracy_reg)
        print("\n")

