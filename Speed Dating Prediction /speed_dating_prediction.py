# -*- coding: utf-8 -*-
"""Speed_Dating_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/146SpKxd1oMhK90-MO1aae1Nkab_O6ru2
"""

# from google.colab import drive
# drive.mount('/content/drive')

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
import numpy as np
from sklearn.feature_selection import SelectKBest, chi2, RFE, SelectFromModel
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
from sklearn.ensemble import AdaBoostClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.exceptions import ConvergenceWarning
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.svm import SVC
from sklearn.decomposition import PCA
import warnings
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from mpl_toolkits.mplot3d import Axes3D
from sklearn.cluster import KMeans

# Reading the dataset
data = pd.read_csv('/content/Copy of speeddating.csv',na_values=["b'?'", ' '])
# data = pd.read_csv('speeddating.csv', na_values=["b'?'", ' '])
#data = pd.read_csv('/content/drive/MyDrive/CS513Project/speeddating.csv',na_values=["b'?'", ' '])

data.head()

"""# Data Cleaning"""

#dropping has_null column since it is has no significance
data.has_null.unique()
data.drop(['has_null'], axis = 1, inplace= True)

#Printing columns with % null values
null_counts=data.isnull().sum().sort_values(ascending=False)
for column, null_count in null_counts.items():
    if null_count > 0:
        print(f"{column}: {round((null_count/data.shape[0])*100,2)}")

# getting to 20 missing value columns
top_20_missing_columns = []
null_counts=data.isnull().sum().sort_values(ascending=False)
count=0
for column, null_count in null_counts.items():
    if null_count > 0 and count<20:
        top_20_missing_columns.append(column)
        count+=1

df_top_20_missing_columns = data[top_20_missing_columns]

plt.figure(figsize=(8, 4))
sns.heatmap(df_top_20_missing_columns.isnull(), cbar=False)

plt.figure(figsize=(10,6))
sns.displot(
    data=df_top_20_missing_columns.isna().melt(value_name="missing"),
    y="variable",
    hue="missing",
    multiple="fill",
    aspect=1.25
)
plt.savefig("visualizing_missing_data_with_barplot_Seaborn_distplot.png", dpi=100)

# If there are more missing values than 15% of the dataset count, drop the columns

temp = []
print('Dropped:')
for col in data.columns:
    if (data[col].isnull().sum()) >= (data.shape[0] *15 / 100):
        print(col, str(data[col].isnull().sum()))
        temp.append(col)

data = data.drop(temp, axis=1)

# All columns with 'd_' except 'd_age' convert values to set ranges. Let's get rid of those columns

temp = []
print('Dropped:')

for col in data.columns:
    if col == 'd_age':
        continue
    if col.startswith('d_'):
        temp.append(col)
        print(col)

data = data.drop(temp, axis=1)

#imputing remaining null values with median
null_counts_revised=data.isnull().sum()

for column, null_count in null_counts_revised.items():
    if null_count > 0:
        print(f"{column}: {null_count}")

# Replace null values with median for numeric columns and highest frequency for object type columns

for col in data.columns:
    if data[col].dtype == 'float64':
        data[col] = data[col].fillna(data[col].median())
    else:
        temp = data[col].value_counts().to_dict()
        print(col)
        print(temp)
        data[col] = data[col].fillna((list(temp.keys())[list(temp.values()).index(data[col].value_counts().max())]))

#Checking null values now.
null_counts_revised=data.isnull().sum()

for column, null_count in null_counts_revised.items():
    if null_count > 0:
        print(f"{column}: {null_count}")

# Slice the string for every object to remove the b''

for col in data:
    if data[col].dtype != 'float64':
        og = data[col].unique()
        rep = []
        for val in og:
            rep.append(str(val)[2:-1])

        data[col].replace(og, rep, inplace=True)

data.head()

"""# Exploratory data analysis


"""

# Divide the age into common age ranges to observe demographics in pie chart
age_pie = {}
age_pie['18-25'] = 0
age_pie['26-30'] = 0
age_pie['31-40'] = 0
age_pie['41+'] = 0

for val in data['age']:
    if val <= 25:
        age_pie['18-25'] += 1
    elif val <=30:
        age_pie['26-30'] += 1
    elif val <= 40:
        age_pie['31-40'] += 1
    else:
        age_pie['41+'] += 1

lab = []
sizes = []
for x, y in age_pie.items():
    lab.append(x)
    sizes.append(y)

_ = plt.pie(sizes, labels= lab,colors=['#d81159', '#8f2d56', '#58586B', '#218380', '#8E9A5E', '#fbb13c'])

# View pie chart of races participating
temp = data['race'].value_counts()
lab = []
sizes = []

for i in range(len(data['race'].unique())):
    lab.append(data['race'].unique()[i])
    sizes.append(temp[data['race'].unique()[i]])

_ = plt.pie(sizes, labels=lab, colors=['#d81159', '#8f2d56', '#58586B', '#218380', '#8E9A5E', '#fbb13c'])

# View bar chart of % matches in each wave

df_temp = data[['wave', 'decision']]

temp = {}

for val in range(data.shape[0]):
    if data['wave'][val] in temp:
        temp[data['wave'][val]] += int(data['match'][val])
    else:
        temp[data['wave'][val]] = int(data['match'][val])

wave_size = data.shape[0] / len(temp)

for key, label in temp.items():
    temp[key] = (label / wave_size) * 100

lab = []
sizes = []

for x, y in temp.items():
    lab.append(x)
    sizes.append(y)

plt.bar(lab, sizes, color = '#8E9A5E')
plt.xticks(lab)
plt.xlabel('Wave')
plt.ylabel('% Matches')
plt.ylim(0, 100)
plt.show()

# Importance vs. rating
# Is a good rating + high importance guarantee of a match?
# Is a low rating + low importance guarantee of a match?

pal = sns.color_palette(['#8F2D56','#FBB13C'])

x = ['attractive_important', 'sincere_important', 'intellicence_important', 'funny_important', 'ambtition_important',
     'shared_interests_important']
y = ['attractive_partner', 'sincere_partner', 'intelligence_partner', 'funny_partner', 'ambition_partner',
     'shared_interests_partner']

plt.figure(figsize=(8, 8))
sns.pairplot(data, x_vars = x, y_vars = y, hue = 'decision', palette = pal)

# Is high rating on qualities of self + high rating on qualities of partner indicative of a match?
# We can remove this. Please check!!!!!!!!
x = ['attractive', 'sincere', 'intelligence', 'funny', 'ambition']
y = ['attractive_partner', 'sincere_partner', 'intelligence_partner', 'funny_partner', 'ambition_partner']

sns.pairplot(data, x_vars = x, y_vars = y, hue = 'decision', palette = pal)

# Plot a boxplot of attractiveness by gender
sns.boxplot(data=data, x='gender', y='attractive_important')

# Plot a boxplot of funniness by gender
sns.boxplot(data=data, x='gender', y='funny_important')

# Plot a boxplot of funniness by gender
sns.boxplot(data=data, x='gender', y='intellicence_important')

# Plot a boxplot of funniness by gender
sns.boxplot(data=data, x='gender', y='ambtition_important')

# Create a violin plot of the distribution of shared_interests_important by gender
sns.violinplot(x='gender', y='shared_interests_important', data=data)
plt.title("Interests by Gender")
plt.xlabel("Gender")
plt.ylabel("Interests")
plt.show()

"""# Encoding and Scaling"""

'''# Dropping decision and descision_o since they are dependent on match column
data.drop(['decision_o','decision'], axis = 1, inplace = True)'''

# Convert object classes into ordinal codes

for col in data.columns:
    if data[col].dtype == 'object':
        oe = OrdinalEncoder()
        oe.fit(data[[col]])
        data[col] = oe.fit_transform(data[[col]])

data.head()

# Drop last two columns since they are not considered in the machine learning codes

data = data.drop(['decision_o', 'match'], axis = 1)

# Divide the data into features and target

X = data.drop(columns = 'decision')  #independent columns
y = data.iloc[:,-1]    #target column

# Normalize the features
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

"""# Feature Selection Methodologies"""

# Pearson Correlation

np.random.seed(20)

def cor_selector(X, y, num_feats):
    cor_list = []
    feature_name = X.columns.tolist()

    # calculate the correlation with y for each feature
    for i in X.columns.tolist():
        cor = np.corrcoef(X[i], y)[0, 1]
        cor_list.append(cor)

    # replace NaN with 0
    cor_list = [0 if np.isnan(i) else i for i in cor_list]

    # feature name
    cor_feature = X.iloc[:, np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist()

    # feature selection? 0 for not select, 1 for select
    cor_support = [True if i in cor_feature else False for i in feature_name]
    return cor_support, cor_feature

cor_support, cor_feature = cor_selector(X, y, len(X.columns))
print(str(len(cor_feature)), 'selected features')
cor_feature.reverse()

k_best_features = 15

# Apply SelectKBest class to extract top k best features

np.random.seed(20)

bestfeatures = SelectKBest(score_func = chi2, k = k_best_features)
fit = bestfeatures.fit(X_scaled, y)

chi_support = fit.get_support()
chi_feature = X.loc[:, chi_support].columns.tolist()
print(str(len(chi_feature)), 'selected features')

# Using RFE

np.random.seed(20)

rfe_selector = RFE(estimator = LogisticRegression(), n_features_to_select = k_best_features, step = 10, verbose = 5)
rfe_selector.fit(X_scaled, y)
rfe_support = rfe_selector.get_support()
rfe_feature = X.loc[:, rfe_support].columns.tolist()
print(str(len(rfe_feature)), 'selected features')

# Logistic Regression to select k best features

np.random.seed(20)
embeded_lr_selector = SelectFromModel(LogisticRegression(penalty = 'l1', solver = 'liblinear'), max_features = k_best_features)
embeded_lr_selector.fit(X_scaled, y)

embeded_lr_support = embeded_lr_selector.get_support()
embeded_lr_feature = X.loc[:, embeded_lr_support].columns.tolist()
print(str(len(embeded_lr_feature)), 'selected features')

# Random Forest to select k best features

np.random.seed(20)
embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators = 100), max_features = k_best_features)
embeded_rf_selector.fit(X, y)

embeded_rf_support = embeded_rf_selector.get_support()
print(len(embeded_rf_support))
embeded_rf_feature = X.loc[:, embeded_rf_support].columns.tolist()
print(str(len(embeded_rf_feature)), 'selected features')

# Combine selections

feature_selection_df = pd.DataFrame({'Feature': X.columns.tolist(), 'Pearson': cor_support, 'Chi-2': chi_support, 'RFE': rfe_support,
                                     'Logistics': embeded_lr_support, 'RandomForest': embeded_rf_support})

# count the selected times for each feature
feature_selection_df['Total'] = np.sum(feature_selection_df, axis = 1)

# display the top 100
feature_selection_df = feature_selection_df.sort_values(['Total'] , ascending = False)
feature_selection_df.index = range(1, len(feature_selection_df) + 1)
feature_selection_df.head(20)

# Select only the top 15 features selected by a combination of all the models

top_15_only = feature_selection_df['Feature'].values.tolist()[0:15]

top_15_only

# Select only the top 15 features to keep in X

X_top = X[top_15_only]
scaler = MinMaxScaler()
X_scaled_15 = scaler.fit_transform(X_top)
X_scaled_15 = pd.DataFrame(X_scaled_15, columns = top_15_only)

X_scaled_15

"""# Resampling
*Not necessary*
"""

# Check if data is imbalanced --> it is not

y.value_counts()

'''#SmoteENN
from imblearn.combine import SMOTEENN
SENN = SMOTEENN()
X_scaled_15_SENN,y_SENN = SENN.fit_resample(X_scaled_15,y)
y_SENN.value_counts()'''

'''from imblearn.over_sampling import ADASYN
ADA= ADASYN()
X_scaled_15_ADA,y_ADA = ADA.fit_resample(X_scaled_15,y)
y_ADA.value_counts()'''

"""# Machine Learning Algorithms"""

X_train, X_test, y_train, y_test = train_test_split(X_scaled_15, y, test_size = 0.3, random_state = 55)

# Decision Tree Classifier

def decision_tree(X_train, X_test, y_train, y_test):
    #Initializing the model
    dec_tree = DecisionTreeClassifier()
    #Fitting the model on training data
    d_tree = dec_tree.fit(X_train,y_train)
    #Predicting the labels values using the test data
    y_pred = dec_tree.predict(X_test)
    #Evaluating the model
    dtree_accuracy = accuracy_score(y_test, y_pred)
    dtree_classification_report = classification_report(y_test, y_pred)
    dtree_f1 = f1_score(y_test, y_pred)
    #Printing the metrics
    print("Accuracy: ", dtree_accuracy)
    print('\n')
    print("F1 Score: ", dtree_f1)
    print('\n')
    print("Confusion Matrix :")
    print(confusion_matrix(y_test, y_pred))
    print('\n')
    print("Classification Report: \n", dtree_classification_report)

    return d_tree, dec_tree

d_tree, dec_tree = decision_tree(X_train, X_test, y_train, y_test)

from sklearn.tree import export_graphviz
import pydotplus
import graphviz
dot_data = export_graphviz(dec_tree, out_file=None,
                           feature_names=X_train.columns,
                           filled=True, rounded=True,
                           special_characters=True)
graph = graphviz.Source(dot_data)
graph.render(directory='doctest-output').replace('\\', '/')
'doctest-output/round-table.gv.pdf'
graph

def adaboost(X_train, X_test, y_train, y_test):
    #Initializing the model
    adaboost = AdaBoostClassifier()
    #Fitting the model on training data
    adaboost_classifier = adaboost.fit(X_train, y_train)
    #Predicting the labels values using the test data
    y_pred = adaboost.predict(X_test)
    #Evaluating the model
    adaboost_accuracy = accuracy_score(y_pred, y_test)
    adaboost_classification_report = classification_report(y_pred, y_test)
    adaboost_f1 = f1_score(y_pred, y_test)
    #Printing the metrics
    print("Accuracy: ", adaboost_accuracy)
    print('\n')
    print("F1 Score: ", adaboost_f1)
    print('\n')
    print("Confusion Matrix :")
    print(confusion_matrix(y_test, y_pred))
    print('\n')
    print("Classification Report: \n", adaboost_classification_report)
    return adaboost_classifier

adaboost_classifier = adaboost(X_train, X_test, y_train, y_test)

def mlp_classifier(X_train, X_test, y_train, y_test):
    mlps=[]
    labels = [0,1]
    max_iter = 5300

    plot_args = [{'c': 'red', 'linestyle': '-'},{'c': 'green', 'linestyle': '-'}]
    #Initializing the model
    mlp = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1, max_iter = max_iter, early_stopping=True, warm_start=True)
    with warnings.catch_warnings():
      warnings.filterwarnings("ignore", category=ConvergenceWarning, module="sklearn")
      mlp.fit(X_train, y_train)
      mlps.append(mlp)
    #Fitting the model on training data
    mlp.fit(X_train, y_train)
    #Predicting the labels values using the test data
    y_pred = mlp.predict(X_test)
    #Evaluating the model
    mlp_accuracy = accuracy_score(y_pred, y_test)
    mlp_classification_report = classification_report(y_pred, y_test)
    mlp_f1 = f1_score(y_pred, y_test)
    #Printing the metrics
    print("Accuracy: ", mlp_accuracy)
    print('\n')
    print("F1 Score: ", mlp_f1)
    print('\n')
    print("Confusion Matrix :")
    print(confusion_matrix(y_test, y_pred))
    print('\n')
    print("Classification Report: \n", mlp_classification_report)

    print("Loss Curve")
    for mlp, label, args in zip(mlps, labels, plot_args):
        plt.plot(mlp.loss_curve_, label=labels, **args)
    plt.show()
    print('Validation Score')
    for mlp, label, args in zip(mlps, labels, plot_args):
        plt.plot(mlp.validation_scores_, label = labels)

    plt.show()

    return mlp


mlp = mlp_classifier(X_train, X_test, y_train, y_test)

from google.colab import drive
drive.mount('/content/drive')

# Random Forest with Randomized Search CV

def Random_forest(X_train, X_test, y_train, y_test):
  param_grid = {
      'n_estimators': [50, 100, 200, 500],
      'max_features': ['sqrt', 'log2'],
      'criterion': ['gini', 'entropy', 'log_loss'],
      'max_depth': [None, 10, 20, 30],
      'min_samples_split': [2, 3, 5, 10],
      'min_samples_leaf': [1, 2, 3, 4]
  }

  rf = RandomForestClassifier()
  rand_search = RandomizedSearchCV(rf, param_distributions = param_grid, n_iter = 10)
  rand_search.fit(X_train, y_train)
  best_model = rand_search.best_estimator_
  print('Tuned Hyperparameters :', rand_search.best_params_)
  y_pred = best_model.predict(X_test)
  best_accuracy = accuracy_score(y_test, y_pred)
  best_f1 = f1_score(y_test, y_pred, pos_label = 0)


  print(f'Accuracy: {best_accuracy}')
  print('\n')
  print(f'F1: {best_f1}')
  print('\n')
  print('Confusion Matrix:')
  print(confusion_matrix(y_test, y_pred))
  print('\n')
  print('Classification Report:')
  print(classification_report(y_test, y_pred))

  return best_model

random_forest_model = Random_forest(X_train, X_test, y_train, y_test)

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

y_prob = random_forest_model.predict_proba(X_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Random guessing')
plt.xlim([-0.05, 1.05])
plt.ylim([-0.05, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

# Random Forest with PCA

def pca_random_forest(X_train, X_test, y_train, y_test):
    pca = PCA(n_components = 3)
    X_train_pca = pca.fit_transform(X_train)
    X_test_pca = pca.transform(X_test)
    rf = RandomForestClassifier()
    rf.fit(X_train_pca, y_train)
    y_pred = rf.predict(X_test_pca)
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average = 'weighted')

    return rf, y_pred, accuracy, f1, X_test_pca

model, y_pred, accuracy, f1, X_test_pca = pca_random_forest(X_train, X_test, y_train, y_test)

print(f'Accuracy: {accuracy}')
print(f'F1 score: {f1}')

from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt

fig = plt.figure(figsize=(8, 8))
ax = fig.add_subplot(111, projection='3d')

# Scatter plot the data points colored by their predicted class
ax.scatter(X_test_pca[:, 0], X_test_pca[:, 1], X_test_pca[:, 2], c=y_pred)

ax.set_xlabel('PC1')
ax.set_ylabel('PC2')
ax.set_zlabel('PC3')
ax.set_title('PCA Random Forest - 3D Scatter Plot')

plt.show()

# Logistic Regression with Grid Search CV

warnings.filterwarnings('ignore')

def LogisticRegressionGridCV(X_train, X_test, y_train, y_test):
    # parameter grid
    parameters = {
        'penalty': ('l1', 'l2', 'elasticnet', None),
        'solver': ('lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'),
        'fit_intercept': (True, False),
        'class_weight': ('balanced', 1)
        }

    logreg = LogisticRegression()
    clf = GridSearchCV(logreg,                    # model
                      param_grid = parameters,   # hyperparameters
                      scoring = 'accuracy',        # metric for scoring
                      cv = 3)
    clf.fit(X_train,y_train)
    print('Tuned Hyperparameters:', clf.best_params_)
    best_model=clf.best_estimator_
    y_pred = best_model.predict(X_test)
    best_accuracy = accuracy_score(y_test, y_pred)
    best_f1 = f1_score(y_test, y_pred, pos_label = 0)
    print(f'Accuracy: {best_accuracy}')
    print('\n')
    print(f'F1: {best_f1}')
    print('\n')
    print('Confusion Matrix:')
    print(confusion_matrix(y_test, y_pred))
    print('\n')
    print('Classification Report:')
    print(classification_report(y_test, y_pred))

    return best_model

lr_gridcv_model = LogisticRegressionGridCV(X_train, X_test, y_train, y_test)

sns.regplot(x = X_test['attractive'], y = y_test, data = data, logistic = True, ci = None, label = 'attractive')
sns.regplot(x = X_test['shared_interests_partner'], y = y_test, data = data, logistic = True, ci = None)
sns.regplot(x = X_test['like'], y = y_test, data = data, logistic = True, ci = None)
sns.regplot(x = X_test['expected_num_matches'], y = y_test, data = data, logistic = True, ci = None)

X_test

# SVM with GridSearchCV

def SVMGridSearch(X_train, X_test, y_train, y_test):
  svm_model = SVC(gamma = 'auto')
  params = {
      'kernel': ('linear', 'poly', 'rbf', 'sigmoid'),
      'degree': (3, 5, 7)
      }
  svm_grid = GridSearchCV(svm_model, param_grid = params, scoring = 'accuracy', cv = 3)
  svm_grid = svm_grid.fit(X_train.to_numpy(), y_train)

  # We obtain the ideal parameters
  print("Tuned Hyperparameters :", svm_grid.best_params_)
  best_model=svm_grid.best_estimator_
  y_pred = best_model.predict(X_test)
  best_accuracy = accuracy_score(y_test, y_pred)
  best_f1 = f1_score(y_test,y_pred, pos_label = 0)
  print(f'Accuracy: {best_accuracy}')
  print('\n')
  print(f'F1: {best_f1}')
  print('\n')
  print('Confusion Matrix:')
  print(confusion_matrix(y_test, y_pred))
  print('\n')
  print('Classification Report:')
  print(classification_report(y_test, y_pred))

  return best_model

svm_grid_model = SVMGridSearch(X_train, X_test, y_train, y_test)

# K-Nearest Neighbors with GridSearchCV

def KNNGridSearchCV(X_train, X_test, y_train, y_test):
    estimator_KNN = KNeighborsClassifier(algorithm = 'auto')
    parameters_KNN = {
    'n_neighbors': [3,5,11,19],
    'weights': ['uniform', 'distance'],
    'metric': ['minkowski', 'euclidean','cosine','manhattan'],
    }

    # with GridSearch
    grid_search_KNN = GridSearchCV(
    estimator = estimator_KNN,
    param_grid = parameters_KNN,
    scoring = 'accuracy',
    n_jobs = -1,
    cv = 10
    )
    grid_search_KNN.fit(X_train, y_train)
    print('Tuned Hyperparameters :', grid_search_KNN.best_params_)
    best_model = grid_search_KNN.best_estimator_
    y_pred = best_model.predict(X_test)
    best_accuracy = accuracy_score(y_test, y_pred)
    best_f1 = f1_score(y_test, y_pred, pos_label = 0)
    print(f'Accuracy: {best_accuracy}')
    print('\n')
    print(f'F1: {best_f1}')
    print('\n')
    print('Confusion Matrix:')
    print(confusion_matrix(y_test, y_pred))
    print('\n')
    print('Classification Report:')
    print(classification_report(y_test, y_pred))

    return best_model

knn_grid_model = KNNGridSearchCV(X_train, X_test, y_train, y_test)

# Naive Bayes

def NaiveBayes(X_train, X_test, y_train, y_test):
  GNBclf = GaussianNB()
  gnb_model = GNBclf.fit(X_train, y_train)
  y_pred = gnb_model.predict(X_test)
  best_accuracy = accuracy_score(y_test, y_pred)
  best_f1 = f1_score(y_test, y_pred, pos_label = 0)
  print(f'Accuracy: {best_accuracy}')
  print('\n')
  print(f'F1: {best_f1}')
  print('\n')
  print('Confusion Matrix:')
  print(confusion_matrix(y_test, y_pred))
  print('\n')
  print('Classification Report:')
  print(classification_report(y_test, y_pred))
  return gnb_model

gnb_model = NaiveBayes(X_train, X_test, y_train, y_test)

# Random Forest with Bagging

def BaggingClassifier_predict(X_train, X_test, y_train, y_test):
    # parameters = {
    #     'n_estimators': [5, 10, 20],
    #     'max_samples': [0.5, 1.0],
    #     'max_features': [0.5, 1.0],
    #     'bootstrap': [True, False],
    #     'bootstrap_features': [True, False]
    # }
    rf = RandomForestClassifier()
    bag_clf = BaggingClassifier(base_estimator = rf)
    # clf = GridSearchCV(bag_clf,                    # model
    #                   param_grid = parameters,     # hyperparameters
    #                   scoring='accuracy')          # metric for scoring
    bag_clf.fit(X_train, y_train)
    # print("Tuned Hyperparameters :", clf.best_params_)
    best_model = bag_clf
    y_pred = best_model.predict(X_test)
    best_accuracy = accuracy_score(y_test, y_pred)
    best_f1 = f1_score(y_test, y_pred, pos_label = 0)
    print(f'Accuracy: {best_accuracy}')
    print('\n')
    print(f'F1: {best_f1}')
    print('\n')
    print('Confusion Matrix:')
    print(confusion_matrix(y_test, y_pred))
    print('\n')
    print('Classification Report:')
    print(classification_report(y_test, y_pred))

    return best_model

bagging_model = BaggingClassifier_predict(X_train, X_test, y_train, y_test)

# KMeans Clustering

def plot_clusters(df, columns, n_clusters):
    # Initialize the KMeans object and fit the data
    kmeans = KMeans(n_clusters = n_clusters)
    kmeans.fit(df[columns])
    labels = kmeans.predict(df[columns])
    df['cluster'] = labels

    # Plot the 3D scatterplot
    fig = plt.figure(figsize = (10, 8))
    ax = fig.add_subplot(111, projection = '3d')
    ax.scatter(df[columns[0]], df[columns[1]], df[columns[2]], c = df['cluster'], cmap = 'viridis')
    centers = kmeans.cluster_centers_
    ax.scatter(centers[:, 0], centers[:, 1], centers[:, 2], marker = 'x', s = 200, linewidths = 3, color = 'r')
    ax.set_xlabel(columns[0])
    ax.set_ylabel(columns[1])
    ax.set_zlabel(columns[2])
    plt.show()

    # Plot the 2D scatterplot
    fig, ax = plt.subplots(figsize = (8, 6))
    ax.scatter(df[columns[0]], df[columns[1]], c = df['cluster'], cmap = 'viridis')
    centers = kmeans.cluster_centers_
    ax.scatter(centers[:, 0], centers[:, 1], marker = 'x', s = 200, linewidths = 3, color = 'r')
    ax.set_xlabel(columns[0])
    ax.set_ylabel(columns[1])
    plt.show()

    return df.groupby('cluster').mean()


X_cluster = X_scaled_15[['like', 'attractive_partner', 'expected_num_matches']]
plot_clusters(X_scaled_15, ['like', 'attractive_partner', 'expected_num_matches'], 3)

# Calculate the accuracy scores for each model

rf_acc = accuracy_score(y_test, random_forest_model.predict(X_test))
pca_rf_acc = accuracy_score(y_test, model.predict(X_test_pca))
logreg_acc = accuracy_score(y_test, lr_gridcv_model.predict(X_test))
knn_acc = accuracy_score(y_test, knn_grid_model.predict(X_test))
gnb_acc = accuracy_score(y_test, gnb_model.predict(X_test))
ada_acc = accuracy_score(y_test, adaboost_classifier.predict(X_test))
svm_acc = accuracy_score(y_test, svm_grid_model.predict(X_test))
mlp_acc = accuracy_score(y_test, mlp_model.predict(X_test))
dt_acc = accuracy_score(y_test, d_tree.predict(X_test))


# Create a bar plot of the accuracy scores
models = ['Random Forest', 'PCA + Random Forest', 'Logistic Regression', 'KNN+GridSearchCV','Naive Bayes','SVM+GridSearcCV','ADABoost','MLP','Decision Tree']
accuracy_scores = [rf_acc, pca_rf_acc, logreg_acc, knn_acc,gnb_acc,svm_acc,ada_acc,mlp_acc,dt_acc]

plt.figure(figsize=(8,6))
plt.bar(models, accuracy_scores)
plt.title('Accuracy Scores for Classification Models')
plt.xlabel('Model')
plt.xticks(rotation = 45,fontsize=8)
plt.ylabel('Accuracy Score')
plt.ylim([0, 1])
plt.show()

